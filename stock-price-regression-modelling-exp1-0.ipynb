{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Stock Price Modelling Regression Experiment 1.0\n","\n","Build baseline models with default parameters. Evaluate their performance. Save all models"]},{"cell_type":"markdown","metadata":{},"source":["# Modelling Regression Experiment #1.0\n","\n","- Split data into Train / Validation / Test set\n","- Create Baseline Models\n","    - 1) Arima\n","        - Test for stationarity\n","        - Find p, q, d terms\n","        - Model Building\n","    - 2) Multivariate Arima\n","        - Causality investigation.\n","        - Test for stationary\n","        - Model Building.\n","        - Test for residuals (errors).\n","        - Forecasting\n","        - Model evaluation\n","    - 3) Random Forest\n","        - Model Building.\n","        - Model evaluation against the test set\n","    - 4) XGBoost\n","        - Model Building.\n","        - Model evaluation against the test set\n","    - 5) kNN\n","        - Model Building.\n","        - Model evaluation against the test set\n","    - 6) SVM\n","        - Model Building.\n","        - Model evaluation against the test set\n","    - 7) LSTM\n","        - Model Building.\n","        - Model evaluation against the test set\n","- Compare their performace"]},{"cell_type":"markdown","metadata":{},"source":["# 1 Install more libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q yfinance==0.2.28\n","!pip install -q pmdarima"]},{"cell_type":"markdown","metadata":{},"source":["# 2 Loading the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:37:41.432624Z","iopub.status.busy":"2023-10-16T13:37:41.43169Z","iopub.status.idle":"2023-10-16T13:37:44.19457Z","shell.execute_reply":"2023-10-16T13:37:44.193419Z","shell.execute_reply.started":"2023-10-16T13:37:41.43259Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","# Suppress all warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# plotting libraries\n","import matplotlib.pyplot as plt\n","%matplotlib inline  \n","import seaborn as sns\n","\n","# ML libraries\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"markdown","metadata":{},"source":["# 3 Download stock data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.16634Z","iopub.status.busy":"2023-10-16T13:38:01.165977Z","iopub.status.idle":"2023-10-16T13:38:01.171768Z","shell.execute_reply":"2023-10-16T13:38:01.170893Z","shell.execute_reply.started":"2023-10-16T13:38:01.166313Z"},"trusted":true},"outputs":[],"source":["def get_stock_data(stock_name):\n","    \n","    # For time stamps\n","    from datetime import datetime\n","    import yfinance as yf\n","    \n","    # The tech stocks we'll use for this analysis\n","    tech_list = [stock_name]\n","\n","    dict_of_stocks = {}\n","\n","    end = datetime.now()\n","    start = datetime(end.year - 10, end.month, end.day)\n","\n","    for stock in tech_list:\n","        globals()[stock] = yf.download(stock, start, end)\n","        dict_of_stocks[tech_list[0]] = globals()[stock]\n","        \n","\n","    return dict_of_stocks[tech_list[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.174304Z","iopub.status.busy":"2023-10-16T13:38:01.173957Z","iopub.status.idle":"2023-10-16T13:38:01.861299Z","shell.execute_reply":"2023-10-16T13:38:01.86032Z","shell.execute_reply.started":"2023-10-16T13:38:01.174272Z"},"trusted":true},"outputs":[],"source":["main_df = get_stock_data(\"Kri.AT\")"]},{"cell_type":"markdown","metadata":{},"source":["# 4 Inspecting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.864089Z","iopub.status.busy":"2023-10-16T13:38:01.863468Z","iopub.status.idle":"2023-10-16T13:38:01.891711Z","shell.execute_reply":"2023-10-16T13:38:01.890285Z","shell.execute_reply.started":"2023-10-16T13:38:01.864064Z"},"trusted":true},"outputs":[],"source":["main_df.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# 4-1 Performance dataframe for models' error/evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_performances = pd.DataFrame({\n","    'rmse': [np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN]\n","}, index=['Arima', 'rf', 'rf_tuned', 'XGBoost', 'XGBoost_tuned', 'svm', 'svm_tuned', 'kNN', 'kNN_tuned', 'LSTM'])"]},{"cell_type":"markdown","metadata":{},"source":["# 4-2 Dataset timestamps"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_start_time = '2023-01-01'\n","train_end_time = '2023-08-31' \n","\n","validation_start_time = '2023-09-01'\n","validation_end_time = '2023-09-30'\n","\n","test_start_time = '2023-10-01'"]},{"cell_type":"markdown","metadata":{},"source":["# 5 Statistical Learning - Time Series"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_dataset_for_arima(main_df, \n","                            train_start = '2020-01-01',\n","                            train_end = '2022-12-31', \n","                            validation_start = '2023-01-01',\n","                            validation_end = '2023-08-31',\n","                            test_start = '2023-09-01'):\n","\n","\n","    # Split the data into train, validation, and test sets\n","    train_set = main_df[(main_df.index >= train_start) & (main_df.index <= train_end)]\n","    validation_set = main_df[(main_df.index >= validation_start) & (main_df.index <= validation_end)]\n","    train_and_validation_set = pd.concat([train_set, validation_set], axis=0)\n","    test_set = main_df[main_df.index >= test_start]\n","\n","\n","    # Print the sizes of the sets\n","    print(\"Train Set Size:\", len(train_set))\n","    print(\"Validation Set Size:\", len(validation_set))\n","    print(\"Test Set Size:\", len(test_set))\n","\n","    return train_set, validation_set, train_and_validation_set, test_set"]},{"cell_type":"markdown","metadata":{},"source":["# 5-1. Spliting the dataset for Arima model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set, validation_set, train_and_validation_set, test_set = split_dataset_for_arima(main_df, \n","                                    train_start = train_start_time,\n","                                    train_end = train_end_time, \n","                                    validation_start = validation_start_time,\n","                                    validation_end = validation_end_time,\n","                                    test_start = test_start_time)"]},{"cell_type":"markdown","metadata":{},"source":["# 6 Baseline Arima"]},{"cell_type":"markdown","metadata":{},"source":["## 6.1 Sources/tutorials\n","\n","https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/\n","\n","https://www.section.io/engineering-education/multivariate-time-series-using-auto-arima/\n"]},{"cell_type":"markdown","metadata":{},"source":["## 6.2 Dickey Fuller Test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.912057Z","iopub.status.busy":"2023-10-16T13:38:01.911588Z","iopub.status.idle":"2023-10-16T13:38:02.226304Z","shell.execute_reply":"2023-10-16T13:38:02.224262Z","shell.execute_reply.started":"2023-10-16T13:38:01.912019Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","from numpy import log\n","result = adfuller(train_set.Close.dropna())\n","print('ADF Statistic: %f' % result[0])\n","print('p-value: %f' % result[1])"]},{"cell_type":"markdown","metadata":{},"source":["If the p-value is less than a significance level (e.g., 0.05), you can reject the null hypothesis, indicating that the time series is stationary.\n","\n","If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting that the time series is non-stationary.\n","\n","The p-value is 0.608091, which is greater than the common significance level of 0.05. Therefore, based on this ADF test, you fail to reject the null hypothesis, suggesting that your time series is likely non-stationary. Non-stationary time series often exhibit trends, seasonality, or other patterns that change over time."]},{"cell_type":"markdown","metadata":{},"source":["## 6-3 How to find the order of differencing (d) in ARIMA model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:02.228897Z","iopub.status.busy":"2023-10-16T13:38:02.228267Z","iopub.status.idle":"2023-10-16T13:38:03.542879Z","shell.execute_reply":"2023-10-16T13:38:03.541721Z","shell.execute_reply.started":"2023-10-16T13:38:02.228864Z"},"trusted":true},"outputs":[],"source":["import numpy as np, pandas as pd\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","import matplotlib.pyplot as plt\n","plt.rcParams.update({'figure.figsize':(19,17), 'figure.dpi':120})\n","\n","\n","# Original Series\n","fig, axes = plt.subplots(3, 2)\n","axes[0, 0].plot(train_set.Close); axes[0, 0].set_title('Original Series')\n","plot_acf(train_set.Close, ax=axes[0, 1])\n","\n","# 1st Differencing\n","axes[1, 0].plot(train_set.Close.diff()); axes[1, 0].set_title('1st Order Differencing')\n","plot_acf(train_set.Close.diff().dropna(), ax=axes[1, 1])\n","\n","# 2nd Differencing\n","axes[2, 0].plot(train_set.Close.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n","plot_acf(train_set.Close.diff().diff().dropna(), ax=axes[2, 1])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 6-4 Adf Test - KPSS test - PP test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:03.544474Z","iopub.status.busy":"2023-10-16T13:38:03.544187Z","iopub.status.idle":"2023-10-16T13:38:04.775052Z","shell.execute_reply":"2023-10-16T13:38:04.774231Z","shell.execute_reply.started":"2023-10-16T13:38:03.544451Z"},"trusted":true},"outputs":[],"source":["from pmdarima.arima.utils import ndiffs\n","\n","## Adf Test\n","adf = ndiffs(train_set.Close, test='adf')  # 1\n","\n","# KPSS test\n","kpss = ndiffs(train_set.Close, test='kpss')  # 1\n","\n","# PP test:\n","pp = ndiffs(train_set.Close, test='pp')  # 1\n","\n","adf, kpss, pp"]},{"cell_type":"markdown","metadata":{},"source":["## 6-5 PACF plot"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:04.779588Z","iopub.status.busy":"2023-10-16T13:38:04.779029Z","iopub.status.idle":"2023-10-16T13:38:05.223469Z","shell.execute_reply":"2023-10-16T13:38:05.221656Z","shell.execute_reply.started":"2023-10-16T13:38:04.779556Z"},"trusted":true},"outputs":[],"source":["# PACF plot of 1st differenced series\n","plt.rcParams.update({'figure.figsize':(19,8), 'figure.dpi':120})\n","\n","fig, axes = plt.subplots(1, 2)\n","axes[0].plot(train_set.Close.diff()); axes[0].set_title('1st Differencing')\n","axes[1].set(ylim=(0,5))\n","plot_pacf(train_set.Close.diff().dropna(), ax=axes[1])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 6-6 Auto Arima to find the best parameters\n","\n","find the optimal p, q, d orders\n","\n","source / tutorial at:\n","https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:05.225206Z","iopub.status.busy":"2023-10-16T13:38:05.224737Z","iopub.status.idle":"2023-10-16T13:38:06.224937Z","shell.execute_reply":"2023-10-16T13:38:06.22397Z","shell.execute_reply.started":"2023-10-16T13:38:05.225154Z"},"trusted":true},"outputs":[],"source":["import pmdarima as pm\n","\n","auto_arima_model = pm.auto_arima(train_set['Close'], start_p=1, d=None, start_q=1, max_p=5, max_d=2, max_q=5, start_P=1, D=None, start_Q=1, max_P=2, max_D=1, max_Q=2, max_order=5, m=1, seasonal=True, stationary=False,error_action='ignore',  \n","                           suppress_warnings=True,\n","                      stepwise=True, trace=True)"]},{"cell_type":"markdown","metadata":{},"source":["## 6-7 Build Arima model after finding AutoArima parameters\n","\n","After using AutoArima, lets build the Arima model using statsmodels library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:06.3051Z","iopub.status.busy":"2023-10-16T13:38:06.304853Z","iopub.status.idle":"2023-10-16T13:38:06.352411Z","shell.execute_reply":"2023-10-16T13:38:06.351334Z","shell.execute_reply.started":"2023-10-16T13:38:06.305075Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.arima.model import ARIMA\n","\n","# Fit the ARIMA model into the test set\n","df = pd.concat([validation_set[-1:], test_set])\n","\n","arima_model = ARIMA(df['Close'], order=(auto_arima_model.order[0], \n","                                        auto_arima_model.order[1], auto_arima_model.order[2]))\n","\n","arima_model_res = arima_model.fit()\n","print(arima_model_res.summary())\n","\n","\n","# create forecasts\n","forecasts =  arima_model_res.predict(start=1, end=len(test_set))\n","\n","# evaluate forecasts against test_set\n","from sklearn.metrics import mean_squared_error\n","\n","mse = mean_squared_error(test_set['Close'], forecasts)\n","rmse = np.sqrt(mse)\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","\n","\n","#plot Close and predicted Close price\n","plot_df = pd.DataFrame({'Date': test_set.index,\n","                    'Close': test_set['Close'],\n","                    'Predicted Close': forecasts\n","                        })\n","\n","\n","my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","plt.figure()\n","my_plot = sns.scatterplot(plot_df)\n","my_plot = sns.lineplot(plot_df)\n","my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","my_plot.set(title='Closing Price vs Predictions')\n","\n","model_performances.loc['Arima'] = rmse\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7 Multivariate Time Series\n","\n","\n","https://blogs.sap.com/2021/05/06/a-multivariate-time-series-modeling-and-forecasting-guide-with-python-machine-learning-client-for-sap-hana/\n","\n","https://towardsdatascience.com/a-quick-introduction-on-granger-causality-testing-for-time-series-analysis-7113dc9420d2#:~:text=The%20Granger%20causality%20test%20is,predict%20tomorrow's%20Tesla's%20stock%20price%3F"]},{"cell_type":"markdown","metadata":{},"source":["## Grangercausalitytests"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:06.363953Z","iopub.status.busy":"2023-10-16T13:38:06.363135Z","iopub.status.idle":"2023-10-16T13:38:11.757063Z","shell.execute_reply":"2023-10-16T13:38:11.756427Z","shell.execute_reply.started":"2023-10-16T13:38:06.363931Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.stattools import grangercausalitytests\n","\n","maxlag=20\n","variables=train_set.columns  \n","matrix = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n","for col in matrix.columns:\n","    for row in matrix.index:\n","        test_result = grangercausalitytests(main_df[[row, col]], maxlag=20, verbose=False)            \n","        p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]            \n","        min_p_value = np.min(p_values)\n","        matrix.loc[row, col] = min_p_value\n","matrix.columns = [var + '_x' for var in variables]\n","matrix.index = [var + '_y' for var in variables]\n","print(matrix)"]},{"cell_type":"markdown","metadata":{},"source":["From the result above, each column represents a predictor x of each variable and each row represents the response y and the p-value of each pair of variables are shown in the matrix. Take the value 0.0212 in (row 1, column 4) as an example, it refers that gdfco_x is causal to rgnp_y. Whereas, the 0.0 in (row 4, column 1) also refers to gdfco_y is the cause of rgnp_x. As all values are all below 0.05 except the diagonal, we could reject that the null hypothesis and this dataset is a good candidate of VectorARIMA modeling."]},{"cell_type":"markdown","metadata":{},"source":["## 7-1 Ad fuller test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:11.760542Z","iopub.status.busy":"2023-10-16T13:38:11.759841Z","iopub.status.idle":"2023-10-16T13:38:12.087552Z","shell.execute_reply":"2023-10-16T13:38:12.086696Z","shell.execute_reply.started":"2023-10-16T13:38:11.760516Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.stattools import adfuller\n","\n","def adfuller_test(df, series, sig=0.05):\n","    res = adfuller(df[series], autolag='AIC')    \n","    p_value = round(res[1], 3)\n","\n","    if p_value <= sig:\n","        print(f\" {series} : P-Value = {p_value} => Stationary. \")\n","    else:\n","        print(f\" {series} : P-Value = {p_value} => Non-stationary.\")\n","\n","for column in train_set.columns:\n","    adfuller_test(main_df, column)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.089545Z","iopub.status.busy":"2023-10-16T13:38:12.088959Z","iopub.status.idle":"2023-10-16T13:38:12.20065Z","shell.execute_reply":"2023-10-16T13:38:12.199817Z","shell.execute_reply.started":"2023-10-16T13:38:12.089516Z"},"trusted":true},"outputs":[],"source":["data_differenced = train_set.diff().dropna()\n","for column in data_differenced.columns:\n","    adfuller_test(data_differenced, column)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.202598Z","iopub.status.busy":"2023-10-16T13:38:12.201995Z","iopub.status.idle":"2023-10-16T13:38:12.347923Z","shell.execute_reply":"2023-10-16T13:38:12.347109Z","shell.execute_reply.started":"2023-10-16T13:38:12.202571Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.api import VAR\n","\n","\n","model = VAR(data_differenced)\n","x = model.select_order(maxlags=21)\n","x.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.349822Z","iopub.status.busy":"2023-10-16T13:38:12.34929Z","iopub.status.idle":"2023-10-16T13:38:12.49575Z","shell.execute_reply":"2023-10-16T13:38:12.494975Z","shell.execute_reply.started":"2023-10-16T13:38:12.349796Z"},"trusted":true},"outputs":[],"source":["from statsmodels.tsa.api import VAR\n","\n","model = VAR(data_differenced)\n","aic = 999\n","best_lag = 0\n","for i in range(1,21):\n","    result = model.fit(i)\n","    if result.aic < aic:\n","        aic = result.aic\n","        best_lag = i\n","\n","print(\"best lag: \", best_lag)"]},{"cell_type":"markdown","metadata":{},"source":["AIC drops at lag order 7. So it is better to keep lag upto 7"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.497567Z","iopub.status.busy":"2023-10-16T13:38:12.497033Z","iopub.status.idle":"2023-10-16T13:38:12.574276Z","shell.execute_reply":"2023-10-16T13:38:12.573485Z","shell.execute_reply.started":"2023-10-16T13:38:12.49754Z"},"trusted":true},"outputs":[],"source":["model_fitted = model.fit(best_lag)\n","model_fitted.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.576122Z","iopub.status.busy":"2023-10-16T13:38:12.57561Z","iopub.status.idle":"2023-10-16T13:38:12.582665Z","shell.execute_reply":"2023-10-16T13:38:12.581864Z","shell.execute_reply.started":"2023-10-16T13:38:12.576097Z"},"trusted":true},"outputs":[],"source":["from statsmodels.stats.stattools import durbin_watson\n","out = durbin_watson(model_fitted.resid)\n","\n"," \n","for col, val in zip(data_differenced.columns, out):\n","    print(col, ':', round(val, 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.584817Z","iopub.status.busy":"2023-10-16T13:38:12.584276Z","iopub.status.idle":"2023-10-16T13:38:12.609641Z","shell.execute_reply":"2023-10-16T13:38:12.608905Z","shell.execute_reply.started":"2023-10-16T13:38:12.584792Z"},"trusted":true},"outputs":[],"source":["lag_order = model_fitted.k_ar\n","#print(lag_order)\n","\n","# Input data for forecasting\n","data_differenced = train_set.diff()\n","forecast_input = data_differenced.values[-lag_order:]\n","#forecast_input\n","\n","# Forecast\n","nobs = validation_set.shape[0]\n","fc = model_fitted.forecast(y=forecast_input, steps=nobs) # nobs defined at top of program\n","df_forecast = pd.DataFrame(fc, index=data_differenced.index[-nobs:], columns=data_differenced.columns + '_1d')\n","df_forecast"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.611465Z","iopub.status.busy":"2023-10-16T13:38:12.610938Z","iopub.status.idle":"2023-10-16T13:38:12.637376Z","shell.execute_reply":"2023-10-16T13:38:12.636516Z","shell.execute_reply.started":"2023-10-16T13:38:12.61144Z"},"trusted":true},"outputs":[],"source":["def invert_transformation(df_train, df_forecast, second_diff=False):\n","    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n","    df_fc = df_forecast.copy()\n","    columns = df_train.columns\n","    for col in columns:        \n","        # Roll back 2nd Diff\n","        if second_diff:\n","            df_fc[str(col)+'_1d'] = (df_train[col].iloc[-1]-df_train[col].iloc[-2]) + df_fc[str(col)+'_2d'].cumsum()\n","        # Roll back 1st Diff\n","        df_fc[str(col)+'_forecast'] = df_train[col].iloc[-1] + df_fc[str(col)+'_1d'].cumsum()\n","    return df_fc\n","\n","df_results = invert_transformation(train_set, df_forecast, second_diff=False)\n","df_results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.639729Z","iopub.status.busy":"2023-10-16T13:38:12.639235Z","iopub.status.idle":"2023-10-16T13:38:15.398858Z","shell.execute_reply":"2023-10-16T13:38:15.39762Z","shell.execute_reply.started":"2023-10-16T13:38:12.639695Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(nrows=int(len(train_set.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n","for i, (col,ax) in enumerate(zip(train_and_validation_set.columns, axes.flatten())):\n","    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True)\n","    train_set[col][-nobs:].plot(legend=True, ax=ax);\n","    ax.set_title(col + \": Forecast vs Actuals\")\n","    ax.xaxis.set_ticks_position('none')\n","    ax.yaxis.set_ticks_position('none')\n","    ax.spines[\"top\"].set_alpha(0)\n","    ax.tick_params(labelsize=6)\n","\n","plt.tight_layout();\n"]},{"cell_type":"markdown","metadata":{},"source":["# 8 Machine Learning models"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the closing price target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:15.400279Z","iopub.status.busy":"2023-10-16T13:38:15.400027Z","iopub.status.idle":"2023-10-16T13:38:15.414075Z","shell.execute_reply":"2023-10-16T13:38:15.412617Z","shell.execute_reply.started":"2023-10-16T13:38:15.400259Z"},"trusted":true},"outputs":[],"source":["main_df['target'] = main_df['Close'].shift(-1)\n","main_df.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# 8-1 Split the dataset for ML"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:15.415716Z","iopub.status.busy":"2023-10-16T13:38:15.415399Z","iopub.status.idle":"2023-10-16T13:38:15.429138Z","shell.execute_reply":"2023-10-16T13:38:15.427285Z","shell.execute_reply.started":"2023-10-16T13:38:15.415689Z"},"trusted":true},"outputs":[],"source":["def split_dataset_for_ml(main_df, \n","                         train_start = '2023-01-01',\n","                         train_end = '2023-08-31', \n","                         validation_start = '2023-09-01',\n","                         validation_end = '2023-09-30',\n","                         test_start = '2023-10-01'):\n","    \n","    \n","    # Prepare the closing price target variable    \n","    main_df['target'] = main_df['Close'].shift(-1)\n","    main_df.tail()\n","\n","\n","    # Split the data into train, validation, and test sets\n","    train_set = main_df[(main_df.index >= train_start) & (main_df.index <= train_end)]\n","    validation_set = main_df[(main_df.index >= validation_start) & (main_df.index <= validation_end)]\n","    train_and_validation_set = pd.concat([train_set, validation_set], axis=0)\n","\n","    test_set = main_df[main_df.index >= test_start]\n","    test_set = test_set[:-1]\n","\n","\n","    # Print the sizes of the sets\n","    print(\"Train Set Size:\", len(train_set))\n","    print(\"Validation Set Size:\", len(validation_set))\n","    print(\"Test Set Size:\", len(test_set))\n","\n","    return train_set, validation_set, train_and_validation_set, test_set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set, validation_set, train_and_validation_set, test_set = split_dataset_for_ml(main_df.copy(), \n","                            train_start = train_start_time,\n","                            train_end = train_end_time, \n","                            validation_start = validation_start_time,\n","                            validation_end = validation_end_time,\n","                            test_start = test_start_time)"]},{"cell_type":"markdown","metadata":{},"source":["# 8-3 Baseline Random Forest Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:48:03.171374Z","iopub.status.busy":"2023-10-16T13:48:03.170114Z","iopub.status.idle":"2023-10-16T13:48:03.850918Z","shell.execute_reply":"2023-10-16T13:48:03.850012Z","shell.execute_reply.started":"2023-10-16T13:48:03.171332Z"},"trusted":true},"outputs":[],"source":["def build_random_forest_baseline_model(train_set, test_set):\n","\n","    from sklearn.ensemble import RandomForestRegressor    \n","    \n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=4)\n","    rf_model.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = rf_model.predict(X_test)\n","    \n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': test_data['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    \n","    # Save the model performance and model's name\n","    model_performances.loc[\"rf\"] = rmse\n","    \n","    return rf_model, rmse\n","\n","rf_model_baseline, rf_model_rmse = build_random_forest_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-3-1 Tuned Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def randomforest_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.ensemble import RandomForestRegressor\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_estimators': hp.choice('n_estimators', range(100, 200, 2)),\n","        'max_depth': hp.choice('max_depth', range(1, 6)),\n","        'min_samples_split': hp.choice('min_samples_split', range(2, 5)),\n","        'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 5)),\n","        'max_features': hp.choice('max_features', ['sqrt', 'log2'])\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = RandomForestRegressor(\n","            n_estimators=params['n_estimators'],\n","            max_depth=params['max_depth'],\n","            min_samples_split=params['min_samples_split'],\n","            min_samples_leaf=params['min_samples_leaf'],\n","            max_features=params['max_features'],\n","            random_state=42,\n","            n_jobs=4\n","        )\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = RandomForestRegressor(**best_params, random_state=42)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    # Save the model performance and model name\n","    model_performances.loc[\"rf_tuned\"] = rmse\n","\n","    return final_model, rmse\n","\n","rf_model_tuned, rf_tuned_rmse = randomforest_tuning(train_set, validation_set, test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 8-4 Baseline XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:16.177774Z","iopub.status.busy":"2023-10-16T13:38:16.176905Z","iopub.status.idle":"2023-10-16T13:38:17.07037Z","shell.execute_reply":"2023-10-16T13:38:17.068984Z","shell.execute_reply.started":"2023-10-16T13:38:16.177737Z"},"trusted":true},"outputs":[],"source":["def build_xgboost_baseline_model(train_set, test_set):\n","\n","    import xgboost as xgb\n","    \n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    xgb_model = xgb.XGBRegressor(n_estimators=100)\n","    xgb_model.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = xgb_model.predict(X_test)\n","    \n","    test_set[\"y_pred\"] = y_pred\n","    \n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model name\n","    model_performances.loc[\"XGBoost\"] = rmse\n","    \n","    return xgb_model, rmse\n","\n","xgb_model_baseline, xgb_model_rmse = build_xgboost_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-4-1 Tuned XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def xgboost_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    import xgboost as xgb\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_estimators': hp.choice('n_estimators', range(100, 400, 2)),\n","        'max_depth': hp.choice('max_depth', range(1, 15)),\n","        'learning_rate':  hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n","        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n","        'min_child_weight': hp.choice('min_child_weight', range(1, 11)),\n","        'gamma': hp.quniform('gamma', 0.4, 1, 0.05),\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = xgb.XGBRegressor(\n","            n_estimators=params['n_estimators'],\n","            max_depth=params['max_depth'],\n","            learning_rate=params['learning_rate'],\n","            subsample=params['subsample'],\n","            min_child_weight=params['min_child_weight'],\n","            gamma=params['gamma'],\n","            objective='reg:squarederror'\n","        )\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = xgb.XGBRegressor(**best_params, random_state=42)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","\n","    # Save the model performance and model name\n","    model_performances.loc[\"XGBoost_tuned\"] = rmse\n","\n","\n","    return final_model, rmse\n","\n","xgboost_model_tuned, xgboost_tuned_rmse = xgboost_tuning(train_set, validation_set, test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 8-5 Baseline SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:17.073362Z","iopub.status.busy":"2023-10-16T13:38:17.072102Z","iopub.status.idle":"2023-10-16T13:38:17.624972Z","shell.execute_reply":"2023-10-16T13:38:17.623963Z","shell.execute_reply.started":"2023-10-16T13:38:17.073318Z"},"trusted":true},"outputs":[],"source":["def build_svm_baseline_model(train_set, test_set):\n","    \n","    from sklearn.svm import SVR\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    svm = SVR(kernel='linear')\n","    svm.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = svm.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model's name\n","    model_performances.loc[\"svm\"] = rmse\n","    \n","    return svm, rmse\n","\n","svm_model_baseline, svm_model_rmse = build_svm_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-5-1 Tuned SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def svr_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.svm import SVR\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n","        'degree':  hp.choice('degree', range(1, 6)),\n","        'C': hp.uniform('C', 0, 1),  # C parameter (log-uniform search space)\n","        'epsilon': hp.loguniform('epsilon', -4, 1),  # Epsilon (log-uniform search space)\n","\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = SVR(**params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = SVR(**best_params)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","\n","    # Save the model performance and model name\n","    model_performances.loc[\"svm_tuned\"] = rmse\n","\n","\n","    return final_model, rmse\n","\n","svm_model_tuned, svm_tuned_rmse = svr_tuning(train_set, validation_set, test_set)"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-10-08T15:55:43.850302Z","iopub.status.busy":"2023-10-08T15:55:43.849547Z","iopub.status.idle":"2023-10-08T15:55:43.887879Z","shell.execute_reply":"2023-10-08T15:55:43.886844Z","shell.execute_reply.started":"2023-10-08T15:55:43.850265Z"}},"source":["# 8-6 Baseline kNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:17.627561Z","iopub.status.busy":"2023-10-16T13:38:17.626783Z","iopub.status.idle":"2023-10-16T13:38:18.111233Z","shell.execute_reply":"2023-10-16T13:38:18.110451Z","shell.execute_reply.started":"2023-10-16T13:38:17.627534Z"},"trusted":true},"outputs":[],"source":["def build_knn_baseline_model(train_set, test_set):\n","    \n","    from sklearn.neighbors import KNeighborsRegressor\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    knn = KNeighborsRegressor(n_neighbors=3)\n","    knn.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = knn.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model's name\n","    model_performances.loc[\"kNN\"] = rmse\n","\n","    # Save the model performance and model name\n","    model_performances.loc[\"kNN_tuned\"] = rmse\n","    \n","    return knn, rmse\n","\n","knn_model_baseline, knn_model_rmse = build_knn_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-6-1 Tuned kNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def kNN_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.neighbors import KNeighborsRegressor\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_neighbors': hp.choice('n_neighbors', range(1, 21)),  # Number of neighbors (integer values)\n","        'weights': hp.choice('weights', ['uniform', 'distance']),\n","        'p': hp.quniform('p', 1, 2, 1),  # Minkowski power parameter (integer values)\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = KNeighborsRegressor(**params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = KNeighborsRegressor(**best_params)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    return final_model, rmse\n","\n","knn_model_tuned, knn_tuned_rmse = kNN_tuning(train_set, validation_set, test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 8-7 Baseline LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:18.112878Z","iopub.status.busy":"2023-10-16T13:38:18.112348Z","iopub.status.idle":"2023-10-16T13:38:33.995489Z","shell.execute_reply":"2023-10-16T13:38:33.994225Z","shell.execute_reply.started":"2023-10-16T13:38:18.11285Z"},"trusted":true},"outputs":[],"source":["def build_lstm_baseline_model(train_set, test_set):\n","    \n","    from tensorflow import keras\n","\n","    # Set a random seed\n","    np.random.seed(42)\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # build the LSTM model\n","    lstm = keras.Sequential()\n","    lstm.add(keras.layers.LSTM(20, input_shape=(X_train.shape[1], 1)))\n","    lstm.add(keras.layers.Dense(1))\n","    \n","    # Compile the model\n","    lstm.compile(optimizer='adam', loss='mean_squared_error')\n","    \n","    # Train the Model\n","    lstm.fit(X_train, y_train, epochs=20, batch_size=30, verbose=0)\n","\n","    # Make predictions using the trained model\n","    y_pred = lstm.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model name\n","    model_performances.loc[\"LSTM\"] = rmse\n","    \n","    return lstm, rmse\n","\n","lstm_model_baseline, lstm_model_rmse = build_lstm_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 9 Evaluate Models' performances"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:33.997894Z","iopub.status.busy":"2023-10-16T13:38:33.99689Z","iopub.status.idle":"2023-10-16T13:38:34.008747Z","shell.execute_reply":"2023-10-16T13:38:34.007765Z","shell.execute_reply.started":"2023-10-16T13:38:33.997848Z"},"trusted":true},"outputs":[],"source":["df_model_performances = pd.DataFrame(model_performances)\n","print(df_model_performances)\n","\n","# Save the dataframe to a CSV file\n","df_model_performances.to_csv('model_performances.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Find the best lag value\n","\n","This is a simple experiment."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:34.01031Z","iopub.status.busy":"2023-10-16T13:38:34.009981Z","iopub.status.idle":"2023-10-16T13:38:34.03078Z","shell.execute_reply":"2023-10-16T13:38:34.029257Z","shell.execute_reply.started":"2023-10-16T13:38:34.010284Z"},"trusted":true},"outputs":[],"source":["def get_series_with_lag(train_set, validation_set, model_str='SVM'):\n","    \n","    import pandas as pd\n","    import numpy as np\n","    \n","    from sklearn.ensemble import RandomForestRegressor\n","    from sklearn.svm import SVR\n","    import xgboost as xgb\n","    from sklearn.neighbors import KNeighborsRegressor\n","    \n","    from sklearn.metrics import mean_squared_error\n","    from sklearn.preprocessing import MinMaxScaler\n","\n","    # Specify the maximum lag period to consider\n","    max_lag_period = 20\n","\n","    # Create empty lists to store lag periods and corresponding RMSE scores\n","    lag_periods = []\n","    rmse_scores = []\n","    \n","    features = train_set.columns\n","    print(model_str)\n","\n","    # Create lag features and evaluate RMSE for each lag period\n","    for lag_period in range(0, max_lag_period + 1):\n","        if lag_period == 0:\n","            \n","            X_train = train_set.drop(['target'], axis=1)\n","            y_train = train_set['target']\n","            X_valid = validation_set.drop(['target'], axis=1)\n","            y_valid = validation_set['target']  \n","            \n","        if lag_period > 0:\n","        # Create lag features\n","            for feature in features:\n","                train_set['{}_Lag_{}'.format(feature, lag_period)] = train_set[feature].shift(lag_period)\n","                validation_set['{}_Lag_{}'.format(feature, lag_period)] = validation_set[feature].shift(lag_period)\n","                \n","                # fill NaN values\n","                train_set.fillna(0, inplace=True)\n","                validation_set.fillna(0, inplace=True)\n","                \n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        X_train = train_data.drop(['target'], axis=1)\n","        y_train = train_data['target']\n","        X_valid = validation_data.drop(['target'], axis=1)\n","        y_valid = validation_data['target']\n","                \n","        \n","        # Train a model\n","        if model_str=='SVM':\n","            model = SVR(kernel='linear')\n","            model.fit(X_train, y_train)\n","        elif model_str==\"RandomForest\":\n","            model = RandomForestRegressor(n_estimators=100, random_state=42)\n","            model.fit(X_train, y_train)\n","        elif model_str==\"XGBoost\":\n","            model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n","            model.fit(X_train, y_train)\n","        elif model_str==\"kNN\":\n","            model = KNeighborsRegressor(n_neighbors=3)\n","            model.fit(X_train, y_train)\n","        else:\n","            print(\"Please select a model like: 'SVM', 'RandomForest', 'XGBoost', 'kNN'\")\n","            return None\n","\n","        # Make predictions on the validation set\n","        y_pred = model.predict(X_valid)\n","        \n","        \n","        # unscale y_pred\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['target'].std()) + train_data['target'].mean()\n","\n","        # Calculate RMSE\n","        rmse = np.sqrt(mean_squared_error(validation_data['target'], y_pred))\n","        print(\"{} rmse with lag {}\".format(rmse, lag_period))\n","\n","        # Store lag period and RMSE\n","        lag_periods.append(lag_period)\n","        rmse_scores.append(rmse)\n","\n","    # Find the best lag period with the lowest RMSE\n","    best_lag_period = lag_periods[np.argmin(rmse_scores)]\n","    best_rmse = rmse_scores[np.argmin(rmse_scores)]\n","\n","    print(\"Best Lag Period: {}\".format(best_lag_period))\n","    print(\"Best RMSE: {}\".format(best_rmse))\n","\n","    return best_lag_period, best_rmse"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:34.032376Z","iopub.status.busy":"2023-10-16T13:38:34.032036Z","iopub.status.idle":"2023-10-16T13:39:09.882944Z","shell.execute_reply":"2023-10-16T13:39:09.88155Z","shell.execute_reply.started":"2023-10-16T13:38:34.032352Z"},"trusted":true},"outputs":[],"source":["# best_lag_period, best_rmse = get_series_with_lag(train_set.copy(), validation_set.copy(), \"RandomForest\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:39:09.884959Z","iopub.status.busy":"2023-10-16T13:39:09.884528Z","iopub.status.idle":"2023-10-16T13:39:16.802631Z","shell.execute_reply":"2023-10-16T13:39:16.801146Z","shell.execute_reply.started":"2023-10-16T13:39:09.884939Z"},"trusted":true},"outputs":[],"source":["# best_lag_period, best_rmse = get_series_with_lag(train_set.copy(), validation_set.copy(), \"SVM\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:39:16.806221Z","iopub.status.busy":"2023-10-16T13:39:16.80537Z","iopub.status.idle":"2023-10-16T13:39:25.205577Z","shell.execute_reply":"2023-10-16T13:39:25.204915Z","shell.execute_reply.started":"2023-10-16T13:39:16.806194Z"},"trusted":true},"outputs":[],"source":["# best_lag_period, best_rmse = get_series_with_lag(train_set.copy(), validation_set.copy(), \"XGBoost\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:39:25.207309Z","iopub.status.busy":"2023-10-16T13:39:25.20688Z","iopub.status.idle":"2023-10-16T13:39:25.878977Z","shell.execute_reply":"2023-10-16T13:39:25.878147Z","shell.execute_reply.started":"2023-10-16T13:39:25.207284Z"},"trusted":true},"outputs":[],"source":["# best_lag_period, best_rmse = get_series_with_lag(train_set.copy(), validation_set.copy(), \"kNN\")"]},{"cell_type":"markdown","metadata":{},"source":["# Save the models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:39:25.880583Z","iopub.status.busy":"2023-10-16T13:39:25.880096Z","iopub.status.idle":"2023-10-16T13:39:25.904026Z","shell.execute_reply":"2023-10-16T13:39:25.902348Z","shell.execute_reply.started":"2023-10-16T13:39:25.880553Z"},"trusted":true},"outputs":[],"source":["import pickle\n","\n","pickle.dump(knn_model_baseline, open('knn_model_baseline.pkl', 'wb'))\n","pickle.dump(svm_model_baseline, open('svm_model_baseline.pkl', 'wb'))\n","pickle.dump(xgb_model_baseline, open('xgb_model_baseline.pkl', 'wb'))\n","pickle.dump(rf_model_baseline, open('rf_model_baseline.pkl', 'wb'))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
