{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["# Stock Price Modelling Regression Experiment 1.0\n","\n","Stock Price modelling - Experimentation Summary:\n","\n","Build baseline models with default parameters. Then tune the same models. Evaluate their performance on Train/Valid/Test set. Save all models and their error performance."]},{"cell_type":"markdown","metadata":{},"source":["# Modelling Regression Experiment #1.0\n","\n","- Split data into Train / Validation / Test set\n","- Create Baseline Models\n","    - 1) Arima\n","        - Test for stationarity\n","        - Find p, q, d terms\n","        - Model Building\n","    - 2) Multivariate Arima\n","        - Causality investigation.\n","        - Test for stationary\n","        - Model Building.\n","        - Test for residuals (errors).\n","        - Forecasting\n","        - Model evaluation\n","    - 3) Random Forest\n","        - Baseline Model with Train set and evaluate on test set.\n","        - Model Tuning with Train set, select best parameters with Validation set and evaluate on test set\n","    - 4) XGBoost\n","        - Baseline Model with Train set and evaluate on test set.\n","        - Model Tuning with Train set, select best parameters with Validation set and evaluate on test set\n","    - 5) kNN\n","        - Baseline Model with Train set and evaluate on test set.\n","        - Model Tuning with Train set, select best parameters with Validation set and evaluate on test set\n","    - 6) SVM\n","        - Baseline Model with Train set and evaluate on test set.\n","        - Model Tuning with Train set, select best parameters with Validation set and evaluate on test set\n","    - 7) LSTM\n","        - Baseline Model with Train set and evaluate on test set.\n","        - Model Tuning with Train set, select best parameters with Validation set and evaluate on test set\n","- Compare their error performace"]},{"cell_type":"markdown","metadata":{},"source":["# Future Ideas / Work"]},{"cell_type":"markdown","metadata":{},"source":["Feature engineering with financial indicators"]},{"cell_type":"markdown","metadata":{},"source":["# 1 Install more libraries\n","\n","Some libraries in remote Kaggle notebook are missing. They need to be installed "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install -q yfinance==0.2.28\n","!pip install -q pmdarima\n","# !pip install -q pandas_datareader"]},{"cell_type":"markdown","metadata":{},"source":["# 2 Loading the libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:37:41.432624Z","iopub.status.busy":"2023-10-16T13:37:41.43169Z","iopub.status.idle":"2023-10-16T13:37:44.19457Z","shell.execute_reply":"2023-10-16T13:37:44.193419Z","shell.execute_reply.started":"2023-10-16T13:37:41.43259Z"},"trusted":true},"outputs":[],"source":["import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O\n","\n","\n","# Suppress all warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# plotting libraries\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","# Sklearn libraries\n","import sklearn\n","from sklearn.metrics import mean_squared_error\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler"]},{"cell_type":"markdown","metadata":{},"source":["# 3 Download stock data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.16634Z","iopub.status.busy":"2023-10-16T13:38:01.165977Z","iopub.status.idle":"2023-10-16T13:38:01.171768Z","shell.execute_reply":"2023-10-16T13:38:01.170893Z","shell.execute_reply.started":"2023-10-16T13:38:01.166313Z"},"trusted":true},"outputs":[],"source":["def get_stock_data(stock_name):\n","    \n","    # For time stamps\n","    from datetime import datetime\n","    import yfinance as yf\n","    \n","    # The tech stocks we'll use for this analysis\n","    tech_list = [stock_name]\n","\n","    dict_of_stocks = {}\n","\n","    end = datetime.now()\n","    start = datetime(end.year - 10, end.month, end.day)\n","\n","    for stock in tech_list:\n","        globals()[stock] = yf.download(stock, start, end)\n","        dict_of_stocks[tech_list[0]] = globals()[stock]\n","        \n","\n","    return dict_of_stocks[tech_list[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.174304Z","iopub.status.busy":"2023-10-16T13:38:01.173957Z","iopub.status.idle":"2023-10-16T13:38:01.861299Z","shell.execute_reply":"2023-10-16T13:38:01.86032Z","shell.execute_reply.started":"2023-10-16T13:38:01.174272Z"},"trusted":true},"outputs":[],"source":["main_df = get_stock_data(\"KRI.AT\")"]},{"cell_type":"markdown","metadata":{},"source":["# 4 Inspecting the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.864089Z","iopub.status.busy":"2023-10-16T13:38:01.863468Z","iopub.status.idle":"2023-10-16T13:38:01.891711Z","shell.execute_reply":"2023-10-16T13:38:01.890285Z","shell.execute_reply.started":"2023-10-16T13:38:01.864064Z"},"trusted":true},"outputs":[],"source":["main_df.head(), main_df.tail()"]},{"cell_type":"markdown","metadata":{},"source":["# 4-1 Performance dataframe for models' error/evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_model_performances = pd.DataFrame({\n","    'rmse': [np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN, np.NAN]\n","}, index=['Arima', 'rf', 'rf_tuned', 'XGBoost', 'XGBoost_tuned', 'svm', 'svm_tuned', 'kNN', 'kNN_tuned', 'LSTM'])"]},{"cell_type":"markdown","metadata":{},"source":["# 4-2 Dataset timestamps"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Experiment_1\n","train_start_time = '2020-01-01'\n","train_end_time = '2023-08-31' \n","\n","validation_start_time = '2023-09-01'\n","validation_end_time = '2023-09-30'\n","\n","test_start_time = '2023-10-01'\n","\n","\n","#Experiment_2\n","\"\"\" \n","train_start_time = '2020-01-01'\n","train_end_time = '2022-12-31' \n","\n","validation_start_time = '2023-01-01'\n","validation_end_time = '2023-09-30'\n","\n","test_start_time = '2023-10-01'\n"," \"\"\"\n","\n","#Experiment_3\n","\"\"\" \n","train_start_time = '2018-01-01'\n","train_end_time = '2022-12-31' \n","\n","validation_start_time = '2023-01-01'\n","validation_end_time = '2023-09-30'\n","\n","test_start_time = '2023-10-01' \"\"\" \n","\n","\n","#Experiment_4\n","\"\"\" \n","train_start_time = str(main_df.index[0].year)+\"-\"+str(main_df.index[0].month)+\"-\"+str(main_df.index[0].day)\n","train_end_time = '2022-12-31' \n","\n","validation_start_time = '2023-01-01'\n","validation_end_time = '2023-09-30'\n","\n","test_start_time = '2023-10-01' \"\"\""]},{"cell_type":"markdown","metadata":{},"source":["# 5 Statistical Learning - Time Series"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_dataset_for_arima(main_df, \n","                            train_start = '2020-01-01',\n","                            train_end = '2022-12-31', \n","                            validation_start = '2023-01-01',\n","                            validation_end = '2023-08-31',\n","                            test_start = '2023-09-01'):\n","\n","\n","    # Split the data into train, validation, and test sets\n","    train_set = main_df[(main_df.index >= train_start) & (main_df.index <= train_end)]\n","    validation_set = main_df[(main_df.index >= validation_start) & (main_df.index <= validation_end)]\n","    train_and_validation_set = pd.concat([train_set, validation_set], axis=0)\n","    test_set = main_df[main_df.index >= test_start]\n","\n","\n","    # Print the sizes of the sets\n","    print(\"Train Set Size:\", len(train_set))\n","    print(\"Validation Set Size:\", len(validation_set))\n","    print(\"Test Set Size:\", len(test_set))\n","\n","    return train_set, validation_set, train_and_validation_set, test_set"]},{"cell_type":"markdown","metadata":{},"source":["# 5-1. Spliting the dataset for Arima model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set, validation_set, train_and_validation_set, test_set = split_dataset_for_arima(main_df, \n","                                    train_start = train_start_time,\n","                                    train_end = train_end_time, \n","                                    validation_start = validation_start_time,\n","                                    validation_end = validation_end_time,\n","                                    test_start = test_start_time)"]},{"cell_type":"markdown","metadata":{},"source":["# 6 Baseline Arima"]},{"cell_type":"markdown","metadata":{},"source":["# 6.1 Sources/tutorials\n","\n","https://www.machinelearningplus.com/time-series/arima-model-time-series-forecasting-python/"]},{"cell_type":"markdown","metadata":{},"source":["# 6.2 Dickey Fuller Test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:01.912057Z","iopub.status.busy":"2023-10-16T13:38:01.911588Z","iopub.status.idle":"2023-10-16T13:38:02.226304Z","shell.execute_reply":"2023-10-16T13:38:02.224262Z","shell.execute_reply.started":"2023-10-16T13:38:01.912019Z"},"trusted":true},"outputs":[],"source":["def dickey_fuller_test(df):\n","\n","    from statsmodels.tsa.stattools import adfuller\n","    from numpy import log\n","    result = adfuller(df.Close.dropna())\n","    print('ADF Statistic: %f' % result[0])\n","    print('p-value: %f' % result[1])\n","\n","dickey_fuller_test(train_set)"]},{"cell_type":"markdown","metadata":{},"source":["If the p-value is less than a significance level (e.g. 0.05), you can reject the null hypothesis, indicating that the time series is stationary.\n","\n","If the p-value is greater than the significance level, you fail to reject the null hypothesis, suggesting that the time series is non-stationary.\n","\n","The p-value is 0.608091, which is greater than the common significance level of 0.05. Therefore, based on this ADF test, you fail to reject the null hypothesis, suggesting that your time series is likely non-stationary. Non-stationary time series often exhibit trends, seasonality, or other patterns that change over time."]},{"cell_type":"markdown","metadata":{},"source":["# 6-3 How to find the order of differencing (d) in ARIMA model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:02.228897Z","iopub.status.busy":"2023-10-16T13:38:02.228267Z","iopub.status.idle":"2023-10-16T13:38:03.542879Z","shell.execute_reply":"2023-10-16T13:38:03.541721Z","shell.execute_reply.started":"2023-10-16T13:38:02.228864Z"},"trusted":true},"outputs":[],"source":["def autocorrelation_plots(df):\n","\n","    from statsmodels.graphics.tsaplots import plot_acf\n","    plt.rcParams.update({'figure.figsize':(19,17), 'figure.dpi':120})\n","\n","    # Original Series\n","    fig, axes = plt.subplots(3, 2)\n","    axes[0, 0].plot(df.Close); axes[0, 0].set_title('Original Series')\n","    plot_acf(df.Close, ax=axes[0, 1])\n","\n","    # 1st Differencing\n","    axes[1, 0].plot(df.Close.diff()); axes[1, 0].set_title('1st Order Differencing')\n","    plot_acf(df.Close.diff().dropna(), ax=axes[1, 1])\n","\n","    # 2nd Differencing\n","    axes[2, 0].plot(df.Close.diff().diff()); axes[2, 0].set_title('2nd Order Differencing')\n","    plot_acf(df.Close.diff().diff().dropna(), ax=axes[2, 1])\n","\n","    plt.show()\n","\n","autocorrelation_plots(train_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 6-4 Adf Test - KPSS test - PP test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:03.544474Z","iopub.status.busy":"2023-10-16T13:38:03.544187Z","iopub.status.idle":"2023-10-16T13:38:04.775052Z","shell.execute_reply":"2023-10-16T13:38:04.774231Z","shell.execute_reply.started":"2023-10-16T13:38:03.544451Z"},"trusted":true},"outputs":[],"source":["from pmdarima.arima.utils import ndiffs\n","\n","## Adf Test\n","adf = ndiffs(train_set.Close, test='adf')  # 1\n","\n","# KPSS test\n","kpss = ndiffs(train_set.Close, test='kpss')  # 1\n","\n","# PP test:\n","pp = ndiffs(train_set.Close, test='pp')  # 1\n","\n","adf, kpss, pp"]},{"cell_type":"markdown","metadata":{},"source":["# 6-5 PACF plot"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:04.779588Z","iopub.status.busy":"2023-10-16T13:38:04.779029Z","iopub.status.idle":"2023-10-16T13:38:05.223469Z","shell.execute_reply":"2023-10-16T13:38:05.221656Z","shell.execute_reply.started":"2023-10-16T13:38:04.779556Z"},"trusted":true},"outputs":[],"source":["def partial_autocorrelation_plots(df):\n","\n","    from statsmodels.graphics.tsaplots import plot_pacf\n","\n","    # PACF plot of 1st differenced series\n","    plt.rcParams.update({'figure.figsize':(19,8), 'figure.dpi':120})\n","\n","    fig, axes = plt.subplots(1, 2)\n","    axes[0].plot(df.Close.diff()); axes[0].set_title('1st Differencing')\n","    axes[1].set(ylim=(0,5))\n","    plot_pacf(df.Close.diff().dropna(), ax=axes[1])\n","\n","    plt.show()\n","\n","partial_autocorrelation_plots(train_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 6-6 Auto Arima to find the best parameters\n","\n","find the optimal p, q, d orders\n","\n","source / tutorial at:\n","https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:05.225206Z","iopub.status.busy":"2023-10-16T13:38:05.224737Z","iopub.status.idle":"2023-10-16T13:38:06.224937Z","shell.execute_reply":"2023-10-16T13:38:06.22397Z","shell.execute_reply.started":"2023-10-16T13:38:05.225154Z"},"trusted":true},"outputs":[],"source":["def auto_arima(df):\n","\n","     import pmdarima as pm\n","\n","     auto_arima_model = pm.auto_arima(df['Close'], start_p=1, d=None, start_q=1, max_p=5, max_d=2, max_q=5, start_P=1, D=None, start_Q=1, max_P=2, max_D=1, max_Q=2, max_order=5, m=1, seasonal=True, stationary=False,error_action='ignore',  \n","                              suppress_warnings=True,\n","                         stepwise=True, trace=True)\n","     \n","     return auto_arima_model\n","\n","auto_arima_model = auto_arima(train_set)\n","\n","          "]},{"cell_type":"markdown","metadata":{},"source":["# 6-7 Build Arima model after finding AutoArima parameters\n","\n","After using AutoArima, lets build the Arima model using statsmodels library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:06.3051Z","iopub.status.busy":"2023-10-16T13:38:06.304853Z","iopub.status.idle":"2023-10-16T13:38:06.352411Z","shell.execute_reply":"2023-10-16T13:38:06.351334Z","shell.execute_reply.started":"2023-10-16T13:38:06.305075Z"},"trusted":true},"outputs":[],"source":["def build_arima_model(auto_arima_model, test_set):\n","\n","    from statsmodels.tsa.arima.model import ARIMA\n","\n","    # Fit the ARIMA model into the test set\n","    df = pd.concat([validation_set[-1:], test_set])\n","\n","    arima_model = ARIMA(df['Close'], order=(auto_arima_model.order[0], \n","                                            auto_arima_model.order[1], \n","                                            auto_arima_model.order[2]))\n","\n","\n","    arima_model_res = arima_model.fit()\n","    print(arima_model_res.summary())\n","\n","\n","    # create forecasts\n","    forecasts =  arima_model_res.predict(start=1, end=len(test_set))\n","\n","    # evaluate forecasts against test_set\n","    from sklearn.metrics import mean_squared_error\n","\n","    mse = mean_squared_error(test_set['Close'], forecasts)\n","    rmse = np.sqrt(mse)\n","    print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","\n","    # Plot residual errors\n","    residuals = pd.DataFrame(arima_model_res.resid)\n","    plt.figure()\n","    fig, ax = plt.subplots(1,2)\n","    residuals.plot(title=\"Residuals\", ax=ax[0])\n","    residuals.plot(kind='kde', title='Density', ax=ax[1])\n","    plt.show()\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                        'Close': test_set['Close'],\n","                        'Predicted Close': forecasts\n","                            })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    df_model_performances.loc['Arima'] = rmse\n","\n","    return arima_model, rmse\n","\n","arima_model, arima_rmse = build_arima_model(auto_arima_model, test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 7 Multivariate Time Series\n","\n","\n","Tutorial / Source:\n","\n","https://www.machinelearningplus.com/time-series/vector-autoregression-examples-python/\n","\n","https://blogs.sap.com/2021/05/06/a-multivariate-time-series-modeling-and-forecasting-guide-with-python-machine-learning-client-for-sap-hana/\n","\n","https://mlpills.dev/time-series/step-by-step-guide-to-multivariate-time-series-forecasting-with-var-models/\n","\n","https://towardsdatascience.com/a-quick-introduction-on-granger-causality-testing-for-time-series-analysis-7113dc9420d2#:~:text=The%20Granger%20causality%20test%20is,predict%20tomorrow's%20Tesla's%20stock%20price%3F"]},{"cell_type":"markdown","metadata":{},"source":["# 7-1 Test for causation amongst the time series \n","\n","Granger Causality tests"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:06.363953Z","iopub.status.busy":"2023-10-16T13:38:06.363135Z","iopub.status.idle":"2023-10-16T13:38:11.757063Z","shell.execute_reply":"2023-10-16T13:38:11.756427Z","shell.execute_reply.started":"2023-10-16T13:38:06.363931Z"},"trusted":true},"outputs":[],"source":["def test_for_causation(train_set):\n","\n","    from statsmodels.tsa.stattools import grangercausalitytests\n","\n","    maxlag = 20\n","    variables=train_set.columns\n","    matrix = pd.DataFrame(np.zeros((len(variables), len(variables))), columns=variables, index=variables)\n","    for col in matrix.columns:\n","        for row in matrix.index:\n","            test_result = grangercausalitytests(train_set[[row, col]], maxlag=20, verbose=False)\n","            p_values = [round(test_result[i+1][0]['ssr_chi2test'][1],4) for i in range(maxlag)]\n","            min_p_value = np.min(p_values)\n","            matrix.loc[row, col] = min_p_value\n","    matrix.columns = [var + '_x' for var in variables]\n","    matrix.index = [var + '_y' for var in variables]\n","    print(matrix)\n","\n","test_for_causation(train_and_validation_set)"]},{"cell_type":"markdown","metadata":{},"source":["From the result above, each column represents a predictor x of each variable and each row represents the response y and the p-value of each pair of variables are shown in the matrix.\n","\n","The row are the response (y) and the columns are the predictors (x). If a given p-value is < significance level (0.05), we can reject the null hypothesis and conclude that Open_x Granger causes High_y."]},{"cell_type":"markdown","metadata":{},"source":["# 7-2 Test for stationarity -  Ad fuller test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:11.760542Z","iopub.status.busy":"2023-10-16T13:38:11.759841Z","iopub.status.idle":"2023-10-16T13:38:12.087552Z","shell.execute_reply":"2023-10-16T13:38:12.086696Z","shell.execute_reply.started":"2023-10-16T13:38:11.760516Z"},"trusted":true},"outputs":[],"source":["def test_for_stationarity(train_set):\n","\n","    from statsmodels.tsa.stattools import adfuller\n","\n","    def adfuller_test(df, series, sig=0.05):\n","        res = adfuller(df[series], autolag='AIC')    \n","        p_value = round(res[1], 3)\n","        \n","        if p_value <= sig:\n","            print(f\" {series} : P-Value = {p_value} => Stationary. \")\n","        else:\n","            print(f\" {series} : P-Value = {p_value} => Non-stationary.\")\n","\n","    for column in train_set.columns:\n","        adfuller_test(train_set, column)\n","\n","test_for_stationarity(train_and_validation_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 7-3 Transform the series to make it stationary, if needed"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.089545Z","iopub.status.busy":"2023-10-16T13:38:12.088959Z","iopub.status.idle":"2023-10-16T13:38:12.20065Z","shell.execute_reply":"2023-10-16T13:38:12.199817Z","shell.execute_reply.started":"2023-10-16T13:38:12.089516Z"},"trusted":true},"outputs":[],"source":["def adfuller_test(df, series, sig=0.05):\n","        \n","        from statsmodels.tsa.stattools import adfuller\n","        res = adfuller(df[series], autolag='AIC')    \n","        p_value = round(res[1], 3)\n","\n","        if p_value <= sig:\n","            print(f\" {series} : P-Value = {p_value} => Stationary. \")\n","        else:\n","            print(f\" {series} : P-Value = {p_value} => Non-stationary.\")\n","\n","\n","#train_set_differenced = train_and_validation_set.diff().dropna()\n","train_set_differenced = train_set.diff().dropna()\n","for column in train_set_differenced.columns:\n","    adfuller_test(train_set_differenced, column)"]},{"cell_type":"markdown","metadata":{},"source":["# 7-4 Model Building"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.202598Z","iopub.status.busy":"2023-10-16T13:38:12.201995Z","iopub.status.idle":"2023-10-16T13:38:12.347923Z","shell.execute_reply":"2023-10-16T13:38:12.347109Z","shell.execute_reply.started":"2023-10-16T13:38:12.202571Z"},"trusted":true},"outputs":[],"source":["def find_best_VAR_order(train_set_differenced):\n","\n","    from statsmodels.tsa.api import VAR\n","\n","    maxlags=21\n","    var_model = VAR(train_set_differenced)\n","    x = var_model.select_order(maxlags)\n","    x.summary()\n","\n","find_best_VAR_order(train_set_differenced)"]},{"cell_type":"markdown","metadata":{},"source":["# 7-5 Find the best lag based on AIC score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.349822Z","iopub.status.busy":"2023-10-16T13:38:12.34929Z","iopub.status.idle":"2023-10-16T13:38:12.49575Z","shell.execute_reply":"2023-10-16T13:38:12.494975Z","shell.execute_reply.started":"2023-10-16T13:38:12.349796Z"},"trusted":true},"outputs":[],"source":["def find_best_lag_order(train_set_differenced):\n","\n","    from statsmodels.tsa.api import VAR\n","\n","    maxlags=20\n","    var_model = VAR(train_set_differenced)\n","    aic = 99999\n","    best_lag = 0\n","    for i in range(1,maxlags+1):\n","        result = var_model.fit(i)\n","        if result.aic < aic:\n","            aic = result.aic\n","            best_lag = i\n","\n","    print(\"best lag: \", best_lag)\n","    return best_lag\n","\n","best_lag = find_best_lag_order(train_set_differenced)"]},{"cell_type":"markdown","metadata":{},"source":["AIC drops at the above lag order. So it is better to keep lag upto that value."]},{"cell_type":"markdown","metadata":{},"source":["# 7-6 Fit the VAR model with best_lag / order"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.497567Z","iopub.status.busy":"2023-10-16T13:38:12.497033Z","iopub.status.idle":"2023-10-16T13:38:12.574276Z","shell.execute_reply":"2023-10-16T13:38:12.573485Z","shell.execute_reply.started":"2023-10-16T13:38:12.49754Z"},"trusted":true},"outputs":[],"source":["import statsmodels.api as sm\n","\n","var_model = sm.tsa.VAR(train_set_differenced)\n","model_var_fitted = var_model.fit(best_lag, ic='aic')\n","model_var_fitted.summary()"]},{"cell_type":"markdown","metadata":{},"source":["# 7-7 Check for Serial Correlation of Residuals (Errors) using Durbin Watson Statistic"]},{"attachments":{"image.png":{"image/png":"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"}},"cell_type":"markdown","metadata":{},"source":["Serial correlation of residuals is used to check if there is any leftover pattern in the residuals (errors).\n","\n","What does this mean to us?\n","\n","If there is any correlation left in the residuals, then, there is some pattern in the time series that is still left to be explained by the model. In that case, the typical course of action is to either increase the order of the model or induce more predictors into the system or look for a different algorithm to model the time series.\n","\n","So, checking for serial correlation is to ensure that the model is sufficiently able to explain the variances and patterns in the time series.\n","\n","Alright, coming back to topic.\n","\n","A common way of checking for serial correlation of errors can be measured using the Durbin Watson’s Statistic.\n","\n","![image.png](attachment:image.png)\n","\n","The value of this statistic can vary between 0 and 4. The closer it is to the value 2, then there is no significant serial correlation. The closer to 0, there is a positive serial correlation, and the closer it is to 4 implies negative serial correlation."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.576122Z","iopub.status.busy":"2023-10-16T13:38:12.57561Z","iopub.status.idle":"2023-10-16T13:38:12.582665Z","shell.execute_reply":"2023-10-16T13:38:12.581864Z","shell.execute_reply.started":"2023-10-16T13:38:12.576097Z"},"trusted":true},"outputs":[],"source":["from statsmodels.stats.stattools import durbin_watson\n","out = durbin_watson(model_var_fitted.resid)\n","\n","\n","for col, val in zip(train_set_differenced.columns, out):\n","    print(col, ':', round(val, 2))"]},{"cell_type":"markdown","metadata":{},"source":["# 7-8 How to Forecast VAR model using statsmodels"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.584817Z","iopub.status.busy":"2023-10-16T13:38:12.584276Z","iopub.status.idle":"2023-10-16T13:38:12.609641Z","shell.execute_reply":"2023-10-16T13:38:12.608905Z","shell.execute_reply.started":"2023-10-16T13:38:12.584792Z"},"trusted":true},"outputs":[],"source":["lag_order = model_var_fitted.k_ar\n","\n","# Input data for forecasting\n","forecast_input = pd.concat([train_and_validation_set[-lag_order-1:], test_set]) # I add -1 cause next line we have dropna which deletes the first case\n","forecast_input_differenced = forecast_input.copy().diff().dropna()\n","forecast_input = pd.concat([train_and_validation_set[-lag_order:], test_set]) # set the corrected forecast_input df having length equal to forecasts without the extra +1 case\n","\n","# Forecast\n","nobs = forecast_input_differenced.shape[0]\n","fc = model_var_fitted.forecast(y = forecast_input_differenced.values, steps = nobs)\n","df_forecast = pd.DataFrame(fc, index = forecast_input_differenced.index, columns = forecast_input.columns + '_1d')"]},{"cell_type":"markdown","metadata":{},"source":["# 7-9 Invert the transformation to get the real forecast"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.611465Z","iopub.status.busy":"2023-10-16T13:38:12.610938Z","iopub.status.idle":"2023-10-16T13:38:12.637376Z","shell.execute_reply":"2023-10-16T13:38:12.636516Z","shell.execute_reply.started":"2023-10-16T13:38:12.61144Z"},"trusted":true},"outputs":[],"source":["def invert_transformation(forecast_input, df_forecast):\n","    \"\"\"Revert back the differencing to get the forecast to original scale.\"\"\"\n","    df_fc = df_forecast.copy()\n","    columns = forecast_input.columns\n","    for col in columns:        \n","        # Roll back 1st Diff\n","        df_fc[str(col)+'_forecast'] = forecast_input[col] + df_fc[str(col)+'_1d'].cumsum()\n","    return df_fc\n","\n","df_results = invert_transformation(forecast_input, df_forecast)\n","df_results.head(lag_order+3)"]},{"cell_type":"markdown","metadata":{},"source":["# 7-10 Plots of Forecasts vs Actuals"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:12.639729Z","iopub.status.busy":"2023-10-16T13:38:12.639235Z","iopub.status.idle":"2023-10-16T13:38:15.398858Z","shell.execute_reply":"2023-10-16T13:38:15.39762Z","shell.execute_reply.started":"2023-10-16T13:38:12.639695Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(nrows=int(len(forecast_input.columns)/2), ncols=2, dpi=150, figsize=(10,10))\n","for i, (col,ax) in enumerate(zip(forecast_input.columns, axes.flatten())):\n","    forecast_input[col].plot(legend=True, ax=ax)\n","    df_results[col+'_forecast'].plot(legend=True, ax=ax).autoscale(axis='x',tight=True);\n","    ax.set_title(col + \": Forecast vs Actuals\")\n","    ax.xaxis.set_ticks_position('none')\n","    ax.yaxis.set_ticks_position('none')\n","    ax.spines[\"top\"].set_alpha(0)\n","    ax.tick_params(labelsize=6)\n","\n","plt.tight_layout();\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7-11 Calculate RMSE for VAR model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["forecasts = df_results[df_results.index >= test_start_time][\"Close_forecast\"]\n","\n","# evaluate forecasts against test_set\n","from sklearn.metrics import mean_squared_error\n","\n","mse = mean_squared_error(test_set['Close'], forecasts)\n","rmse = np.sqrt(mse)\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","\n","\n","#plot Close and predicted Close price\n","plot_df = pd.DataFrame({'Date': test_set.index,\n","                    'Close': test_set['Close'],\n","                    'Predicted Close': forecasts\n","                        })\n","\n","\n","my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","plt.figure()\n","my_plot = sns.scatterplot(plot_df)\n","my_plot = sns.lineplot(plot_df)\n","my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","my_plot.set(title='Closing Price vs Predictions')\n","\n","df_model_performances.loc['VAR'] = rmse"]},{"cell_type":"markdown","metadata":{},"source":["# 8 Machine Learning models"]},{"cell_type":"markdown","metadata":{},"source":["# Prepare the closing price target variable"]},{"cell_type":"markdown","metadata":{},"source":["# 8-1 Split the dataset for ML"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:15.415716Z","iopub.status.busy":"2023-10-16T13:38:15.415399Z","iopub.status.idle":"2023-10-16T13:38:15.429138Z","shell.execute_reply":"2023-10-16T13:38:15.427285Z","shell.execute_reply.started":"2023-10-16T13:38:15.415689Z"},"trusted":true},"outputs":[],"source":["def split_dataset_for_ml(main_df, \n","                         train_start = '2023-01-01',\n","                         train_end = '2023-08-31', \n","                         validation_start = '2023-09-01',\n","                         validation_end = '2023-09-30',\n","                         test_start = '2023-10-01'):\n","    \n","    \n","    # Prepare the closing price as target variable    \n","    main_df['target'] = main_df['Close'].shift(-1)\n","\n","\n","    # Split the data into train, validation, and test sets\n","    train_set = main_df[(main_df.index >= train_start) & (main_df.index <= train_end)]\n","    validation_set = main_df[(main_df.index >= validation_start) & (main_df.index <= validation_end)]\n","    train_and_validation_set = pd.concat([train_set, validation_set], axis=0)\n","\n","    test_set = main_df[main_df.index >= test_start]\n","    test_set = test_set[:-1]\n","\n","\n","    # Print the sizes of the sets\n","    print(\"Train Set Size:\", len(train_set))\n","    print(\"Validation Set Size:\", len(validation_set))\n","    print(\"Test Set Size:\", len(test_set))\n","\n","    return train_set, validation_set, train_and_validation_set, test_set"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_set, validation_set, train_and_validation_set, test_set = split_dataset_for_ml(main_df.copy(), \n","                            train_start = train_start_time,\n","                            train_end = train_end_time, \n","                            validation_start = validation_start_time,\n","                            validation_end = validation_end_time,\n","                            test_start = test_start_time)"]},{"cell_type":"markdown","metadata":{},"source":["# 8-3 Baseline Random Forest Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:48:03.171374Z","iopub.status.busy":"2023-10-16T13:48:03.170114Z","iopub.status.idle":"2023-10-16T13:48:03.850918Z","shell.execute_reply":"2023-10-16T13:48:03.850012Z","shell.execute_reply.started":"2023-10-16T13:48:03.171332Z"},"trusted":true},"outputs":[],"source":["def build_random_forest_baseline_model(train_set, test_set):\n","\n","    from sklearn.ensemble import RandomForestRegressor    \n","    \n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=4)\n","    rf_model.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = rf_model.predict(X_test)\n","    \n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': test_data['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    \n","    # Save the model performance and model's name\n","    df_model_performances.loc[\"rf\"] = rmse\n","    \n","    return rf_model, rmse\n","\n","rf_model_baseline, rf_model_rmse = build_random_forest_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-3-1 Tuned Random Forest"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def randomforest_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.ensemble import RandomForestRegressor\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_estimators': hp.choice('n_estimators', range(100, 200, 2)),\n","        'max_depth': hp.choice('max_depth', range(1, 6)),\n","        'min_samples_split': hp.choice('min_samples_split', range(2, 5)),\n","        'min_samples_leaf': hp.choice('min_samples_leaf', range(1, 5)),\n","        'max_features': hp.choice('max_features', ['sqrt', 'log2'])\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = RandomForestRegressor(\n","            n_estimators=params['n_estimators'],\n","            max_depth=params['max_depth'],\n","            min_samples_split=params['min_samples_split'],\n","            min_samples_leaf=params['min_samples_leaf'],\n","            max_features=params['max_features'],\n","            random_state=42,\n","            n_jobs=4\n","        )\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = RandomForestRegressor(**best_params, random_state=42)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    # Save the model performance and model name\n","    df_model_performances.loc[\"rf_tuned\"] = rmse\n","\n","    return final_model, rmse\n","\n","rf_model_tuned, rf_tuned_rmse = randomforest_tuning(train_set.copy(), validation_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-4 Baseline XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:16.177774Z","iopub.status.busy":"2023-10-16T13:38:16.176905Z","iopub.status.idle":"2023-10-16T13:38:17.07037Z","shell.execute_reply":"2023-10-16T13:38:17.068984Z","shell.execute_reply.started":"2023-10-16T13:38:16.177737Z"},"trusted":true},"outputs":[],"source":["def build_xgboost_baseline_model(train_set, test_set):\n","\n","    import xgboost as xgb\n","    \n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    xgb_model = xgb.XGBRegressor(n_estimators=100)\n","    xgb_model.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = xgb_model.predict(X_test)\n","    \n","    test_set[\"y_pred\"] = y_pred\n","    \n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model name\n","    df_model_performances.loc[\"XGBoost\"] = rmse\n","    \n","    return xgb_model, rmse\n","\n","xgb_model_baseline, xgb_model_rmse = build_xgboost_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-4-1 Tuned XGBoost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def xgboost_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    import xgboost as xgb\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_estimators': hp.choice('n_estimators', range(100, 400, 2)),\n","        'max_depth': hp.choice('max_depth', range(1, 15)),\n","        'learning_rate':  hp.quniform('learning_rate', 0.025, 0.5, 0.025),\n","        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n","        'min_child_weight': hp.choice('min_child_weight', range(1, 11)),\n","        'gamma': hp.quniform('gamma', 0.4, 1, 0.05),\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = xgb.XGBRegressor(\n","            n_estimators=params['n_estimators'],\n","            max_depth=params['max_depth'],\n","            learning_rate=params['learning_rate'],\n","            subsample=params['subsample'],\n","            min_child_weight=params['min_child_weight'],\n","            gamma=params['gamma'],\n","            objective='reg:squarederror'\n","        )\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = xgb.XGBRegressor(**best_params, random_state=42)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","\n","    # Save the model performance and model name\n","    df_model_performances.loc[\"XGBoost_tuned\"] = rmse\n","\n","\n","    return final_model, rmse\n","\n","xgb_model_tuned, xgboost_tuned_rmse = xgboost_tuning(train_set.copy(), validation_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-5 Baseline SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:17.073362Z","iopub.status.busy":"2023-10-16T13:38:17.072102Z","iopub.status.idle":"2023-10-16T13:38:17.624972Z","shell.execute_reply":"2023-10-16T13:38:17.623963Z","shell.execute_reply.started":"2023-10-16T13:38:17.073318Z"},"trusted":true},"outputs":[],"source":["def build_svm_baseline_model(train_set, test_set):\n","    \n","    from sklearn.svm import SVR\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    svm = SVR(kernel='linear')\n","    svm.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = svm.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model's name\n","    df_model_performances.loc[\"svm\"] = rmse\n","    \n","    return svm, rmse\n","\n","svm_model_baseline, svm_model_rmse = build_svm_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-5-1 Tuned SVM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def svr_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.svm import SVR\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'kernel': hp.choice('kernel', ['linear', 'rbf', 'sigmoid']),\n","        'degree':  hp.choice('degree', range(1, 6)),\n","        'C': hp.uniform('C', 0, 1),  # C parameter (log-uniform search space)\n","        'epsilon': hp.loguniform('epsilon', -4, 1),  # Epsilon (log-uniform search space)\n","\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = SVR(**params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = SVR(**best_params)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","\n","    # Save the model performance and model name\n","    df_model_performances.loc[\"svm_tuned\"] = rmse\n","\n","\n","    return final_model, rmse\n","\n","svm_model_tuned, svm_tuned_rmse = svr_tuning(train_set.copy(), validation_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-10-08T15:55:43.850302Z","iopub.status.busy":"2023-10-08T15:55:43.849547Z","iopub.status.idle":"2023-10-08T15:55:43.887879Z","shell.execute_reply":"2023-10-08T15:55:43.886844Z","shell.execute_reply.started":"2023-10-08T15:55:43.850265Z"}},"source":["# 8-6 Baseline kNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:17.627561Z","iopub.status.busy":"2023-10-16T13:38:17.626783Z","iopub.status.idle":"2023-10-16T13:38:18.111233Z","shell.execute_reply":"2023-10-16T13:38:18.110451Z","shell.execute_reply.started":"2023-10-16T13:38:17.627534Z"},"trusted":true},"outputs":[],"source":["def build_knn_baseline_model(train_set, test_set):\n","    \n","    from sklearn.neighbors import KNeighborsRegressor\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # Train the Random Forest model\n","    knn = KNeighborsRegressor(n_neighbors=3)\n","    knn.fit(X_train, y_train)\n","\n","    # Make predictions using the trained model\n","    y_pred = knn.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model's name\n","    df_model_performances.loc[\"kNN\"] = rmse\n","    \n","    return knn, rmse\n","\n","knn_model_baseline, knn_model_rmse = build_knn_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-6-1 Tuned kNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def kNN_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    from sklearn.neighbors import KNeighborsRegressor\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","        'n_neighbors': hp.choice('n_neighbors', range(1, 21)),  # Number of neighbors (integer values)\n","        'weights': hp.choice('weights', ['uniform', 'distance']),\n","        'p': hp.quniform('p', 1, 2, 1),  # Minkowski power parameter (integer values)\n","    }\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = StandardScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = KNeighborsRegressor(**params)\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = StandardScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = KNeighborsRegressor(**best_params)\n","    final_model.fit(X_train, y_train)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * train_data['Close'].std()) + train_data['Close'].mean()\n","    y_test = test_data['target']\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': y_pred\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    # Save the model performance and model's name\n","    df_model_performances.loc[\"kNN_tuned\"] = rmse\n","\n","    return final_model, rmse\n","\n","knn_model_tuned, knn_tuned_rmse = kNN_tuning(train_set.copy(), validation_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-7 Baseline LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:18.112878Z","iopub.status.busy":"2023-10-16T13:38:18.112348Z","iopub.status.idle":"2023-10-16T13:38:33.995489Z","shell.execute_reply":"2023-10-16T13:38:33.994225Z","shell.execute_reply.started":"2023-10-16T13:38:18.11285Z"},"trusted":true},"outputs":[],"source":["def build_lstm_baseline_model(train_set, test_set):\n","    \n","    from tensorflow import keras\n","    from sklearn.preprocessing import MinMaxScaler\n","\n","    # Set a random seed\n","    np.random.seed(42)\n","\n","    #scaling the data before modeling\n","    scaler = MinMaxScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","    \n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","    \n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","    \n","    \n","    # build the LSTM model\n","    lstm = keras.Sequential()\n","    lstm.add(keras.layers.LSTM(20, input_shape=(X_train.shape[1], 1)))\n","    lstm.add(keras.layers.Dense(1))\n","    \n","    # Compile the model\n","    lstm.compile(optimizer='adam', loss='mean_squared_error')\n","    \n","    # Train the Model\n","    lstm.fit(X_train, y_train, epochs=20, batch_size=30, verbose=0)\n","\n","    # Make predictions using the trained model\n","    y_pred = lstm.predict(X_test)\n","    \n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * (train_data['Close'].max() - train_data['Close'].min())) + train_data['Close'].min()\n","    \n","    test_data[\"y_pred\"] = y_pred\n","    \n","    \n","    # Calculate RMSE (Root Mean Squared Error) to evaluate the model's performance\n","    rmse = np.sqrt(mean_squared_error(test_data[\"target\"], test_data[\"y_pred\"]))\n","    print(f\"RMSE: {rmse}\")\n","\n","\n","    #plot Close and predicted Close price\n","    \n","    plot_df = pd.DataFrame({'Date': test_set.index,\n","                       'Close': test_set['target'],\n","                       'Predicted Close': test_set['y_pred']\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","    \n","    # Save the model performance and model name\n","    df_model_performances.loc[\"LSTM\"] = rmse\n","    \n","    return lstm, rmse\n","\n","lstm_model_baseline, lstm_model_rmse = build_lstm_baseline_model(train_set.copy(), test_set.copy())"]},{"cell_type":"markdown","metadata":{},"source":["# 8-7-1 Tuned LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def LSTM_tuning(train_set, validation_set, test_set):\n","\n","    from hyperopt import hp, tpe, fmin, Trials, space_eval\n","    import keras\n","    from keras.models import Sequential\n","    from keras.layers import LSTM, Dense\n","    from sklearn.preprocessing import MinMaxScaler\n","\n","    # Set a random seed\n","    ''' \n","    By setting the random seed with np.random.seed(42), \n","    you ensure that the random sampling in Hyperopt is reproducible across runs \n","    as long as the seed remains the same.\n","    '''\n","    np.random.seed(42)\n","\n","    # Define the search space for hyperparameters\n","    space = {\n","    'lstm_units': hp.choice('lstm_units', range(5, 51, 2)),\n","    'learning_rate': hp.quniform('learning_rate', 0.025, 0.5, 0.005),\n","    'batch_size': hp.choice('batch_size', range(8, 65, 4)),\n","    'epochs': hp.choice('epochs', range(10, 51, 5))\n","}\n","\n","    # Define the objective function to minimize (negative mean squared error)\n","    def objective(params):\n","\n","        #scaling the data before modeling\n","        scaler = MinMaxScaler()\n","        scaled_array = scaler.fit_transform(train_set)\n","        train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","        \n","        scaled_array = scaler.transform(validation_set)\n","        validation_data = pd.DataFrame(scaled_array, columns=validation_set.columns, index=validation_set.index)\n","\n","        \n","        # Prepare the data for Random Forest\n","        X_train = train_data.drop(['target'], axis=1, inplace=False)\n","        y_train = train_data['target']\n","\n","        X_test = validation_data.drop(['target'], axis=1, inplace=False)\n","        y_test = validation_data['target']\n","\n","        model = Sequential()\n","        model.add(LSTM(units=params['lstm_units'], input_shape=(X_train.shape[1], 1)))\n","        model.add(Dense(1))\n","        model.compile(optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']), loss='mean_squared_error')\n","        model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0)\n","        y_pred = model.predict(X_test)\n","\n","        # unscale y_pred_diff\n","        train_data = train_set\n","        validation_data = validation_set\n","        y_pred = (y_pred * (train_data['Close'].max() - train_data['Close'].min())) + train_data['Close'].min()\n","        y_test = validation_data['target']\n","\n","        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","        return rmse\n","\n","    # Create an instance of the Trials class to keep track of the results\n","    trials = Trials()\n","\n","    # Run the hyperparameter search using the TPE algorithm\n","    best = fmin(fn=objective,\n","                space=space,\n","                algo=tpe.suggest,\n","                max_evals=250,\n","                trials=trials,\n","                rstate=np.random.default_rng(42))\n","\n","\n","    # Get the best hyperparameters\n","    best_params = space_eval(space, best)\n","    print(best_params)\n","\n","    # Train the final model with the best hyperparameters\n","\n","    #scaling the data before modeling\n","    scaler = MinMaxScaler()\n","    scaled_array = scaler.fit_transform(train_set)\n","    train_data = pd.DataFrame(scaled_array, columns=train_set.columns, index=train_set.index)\n","\n","    scaled_array = scaler.transform(test_set)\n","    test_data = pd.DataFrame(scaled_array, columns=test_set.columns, index=test_set.index)\n","\n","\n","    # Prepare the data for Random Forest\n","    X_train = train_data.drop(['target'], axis=1, inplace=False)\n","    y_train = train_data['target']\n","\n","    X_test = test_data.drop(['target'], axis=1, inplace=False)\n","    y_test = test_data['target']\n","\n","\n","    final_model = Sequential()\n","    final_model.add(LSTM(units=best_params['lstm_units'], input_shape=(X_train.shape[1], 1)))\n","    final_model.add(Dense(1))\n","    final_model.compile(optimizer=keras.optimizers.Adam(learning_rate=best_params['learning_rate']), loss='mean_squared_error')\n","    final_model.fit(X_train, y_train, epochs=best_params['epochs'], batch_size=best_params['batch_size'], verbose=0)\n","\n","    # Make predictions on the test set\n","    y_pred = final_model.predict(X_test)\n","\n","    # unscale y_pred_diff\n","    train_data = train_set\n","    test_data = test_set\n","    y_pred = (y_pred * (train_data['Close'].max() - train_data['Close'].min())) + train_data['Close'].min()\n","    y_test = test_data['target']\n","    test_data[\"y_pred\"] = y_pred\n","\n","    # Calculate RMSE\n","    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n","    print(\"{} rmse\".format(rmse))\n","\n","\n","    #plot Close and predicted Close price\n","    plot_df = pd.DataFrame({'Date': test_data.index,\n","                       'Close': test_data['target'],\n","                       'Predicted Close': test_data[\"y_pred\"]\n","                           })\n","\n","\n","    my_plot = sns.set(rc={'figure.figsize':(11, 3)})\n","    plt.figure()\n","    my_plot = sns.scatterplot(plot_df)\n","    my_plot = sns.lineplot(plot_df)\n","    my_plot.set_xticklabels(my_plot.get_xticklabels(), rotation=45)\n","    my_plot.set(title='Closing Price vs Predictions')\n","\n","    # Save the model performance and model's name\n","    df_model_performances.loc[\"LSTM_tuned\"] = rmse\n","\n","    return final_model, rmse\n","\n","lstm_model_tuned, lstm_tuned_rmse = LSTM_tuning(train_set, validation_set, test_set)"]},{"cell_type":"markdown","metadata":{},"source":["# 9 Evaluate Models' performances"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:38:33.997894Z","iopub.status.busy":"2023-10-16T13:38:33.99689Z","iopub.status.idle":"2023-10-16T13:38:34.008747Z","shell.execute_reply":"2023-10-16T13:38:34.007765Z","shell.execute_reply.started":"2023-10-16T13:38:33.997848Z"},"trusted":true},"outputs":[],"source":["df_model_performances = df_model_performances.sort_values(by='rmse')\n","print(df_model_performances.sort_values(by='rmse'))\n","\n","# Save the dataframe to a CSV file\n","df_model_performances.to_csv('df_model_performances_exp1.0.csv', index=True)"]},{"cell_type":"markdown","metadata":{},"source":["# 10 Save the models"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pickle\n","\n","\n","pickle.dump(model_var_fitted, open('var_model_tuned_exp_1.0.pkl', 'wb'))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-16T13:39:25.880583Z","iopub.status.busy":"2023-10-16T13:39:25.880096Z","iopub.status.idle":"2023-10-16T13:39:25.904026Z","shell.execute_reply":"2023-10-16T13:39:25.902348Z","shell.execute_reply.started":"2023-10-16T13:39:25.880553Z"},"trusted":true},"outputs":[],"source":["import pickle\n","import os\n","\n","\"\"\" # directory we want to create\n","directory_path = '/models'\n","\n","# Check if the directory already exists\n","if not os.path.exists(directory_path):\n","    # If it doesn't exist, create it\n","    os.makedirs(directory_path) \"\"\"\n","\n","pickle.dump(knn_model_baseline, open('knn_model_baseline_exp_1.0.pkl', 'wb'))\n","pickle.dump(svm_model_baseline, open('svm_model_baseline_exp_1.0.pkl', 'wb'))\n","pickle.dump(xgb_model_baseline, open('xgb_model_baseline_exp_1.0.pkl', 'wb'))\n","pickle.dump(rf_model_baseline, open('rf_model_baseline_exp_1.0.pkl', 'wb'))\n","\n","pickle.dump(knn_model_tuned, open('knn_model_tuned_exp_1.0.pkl', 'wb'))\n","pickle.dump(svm_model_tuned, open('svm_model_tuned_exp_1.0.pkl', 'wb'))\n","pickle.dump(xgb_model_tuned, open('xgb_model_tuned_exp_1.0.pkl', 'wb'))\n","pickle.dump(rf_model_tuned, open('rf_model_tuned_exp_1.0.pkl', 'wb'))"]},{"cell_type":"markdown","metadata":{},"source":["# 11 Create Experiment info file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","\n","def create_file_and_write_string_values(file_name, string_values):\n","  \"\"\"Creates a file and writes the given string values inside.\n","\n","  Args:\n","    file_name: The name of the file to create.\n","    string_values: A list of string values to write to the file.\n","  \"\"\"\n","\n","  # If the file exists, delete it.\n","  if os.path.exists(file_name):\n","    os.remove(file_name)\n","\n","  # Open the file for writing.\n","  with open(file_name, \"w\") as f:\n","    for string_value in string_values:\n","      f.write(string_value + \"\\n\")\n","\n","\n","create_file_and_write_string_values(\"exp1.0-timestamps.txt\", \n","                                    [\"train_start_time:\", train_start_time, \n","                                     \"train_end_time:\", train_end_time, \n","                                     \"validation_start_time:\", validation_start_time, \n","                                     \"validation_end_time:\", validation_end_time, \n","                                     \"test_start_time:\", test_start_time])"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
